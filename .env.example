# Database
DATABASE_URL="postgresql://user:password@localhost:5432/pc_parts_db?schema=public"

# Application
NODE_ENV="development"
NEXT_PUBLIC_APP_URL="http://localhost:3000"

# NextAuth Authentication
# Generate a secret key with: openssl rand -base64 32
NEXTAUTH_URL="http://localhost:3000"
NEXTAUTH_SECRET="your-secret-key-here-generate-with-openssl-rand-base64-32"

# Email Configuration (for verification emails)
# Gmail OAuth2 (used in production by nodemailer OAuth2 transport)
GMAIL_OAUTH_USER="email@sample.com"
GOOGLE_CLIENT_ID="secret-key-here"
GOOGLE_CLIENT_SECRET="secret-key-here"
GOOGLE_REFRESH_TOKEN="secret-key-here"
GOOGLE_OAUTH_REDIRECT_URL="http://localhost:3000/oauth/callback"

# Scraping Configuration
SCRAPER_TIMEOUT=30000
SCRAPER_MAX_RETRIES=3
SCRAPER_USER_AGENT="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"

# Scheduler Configuration
# Set to "true" to enable automatic scraping every 3 hours, "false" to disable
# Use this for local development or VPS deployments with persistent processes
ENABLE_SCHEDULER="false"

# Cron Configuration
# Set to "true" to enable cron endpoint for automatic scraping, "false" to disable
# Use this for Vercel (serverless) deployments
# Note: For Vercel, you need both this set to "true" AND cron configured in vercel.json
ENABLE_CRON="true"

# Retailer-specific scraping limits
# Set to 0 or -1 for unlimited pages
# PCWORTH: Max pages to scrape per category per branch
PCWORTH_MAX_PAGES=50
# PCWORTH: API items per page limit
PCWORTH_ITEMS_PER_PAGE=48
# BERMOR: Max pages to scrape per category
BERMOR_MAX_PAGES=50
# DATABLITZ: Max pages to scrape per category (currently processes all available)
DATABLITZ_MAX_PAGES=50

# Rate Limiting
RATE_LIMIT_REQUESTS_PER_MINUTE=60

# Cron Job Configuration (for Vercel Cron)
# Generate a strong secret with: openssl rand -base64 32
CRON_SECRET="your-secret-key-here-generate-with-openssl-rand-base64-32"

# Logging
LOG_LEVEL="info"
